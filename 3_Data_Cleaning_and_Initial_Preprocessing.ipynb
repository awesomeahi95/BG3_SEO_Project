{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e781b118-f049-4149-80d3-d54cbd3934d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning and Initial Preprocessing\n",
    "---\n",
    "\n",
    "## <ins>Objective</ins>\n",
    "The purpose of this notebook is to clean and preprocess the raw data extracted from guide articles about Baldur's Gate 3. This includes handling missing or duplicate values, cleaning the raw text data, and standardizing the meta tag columns. These steps will prepare the data for further analysis and advanced preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "## <ins>Workflow</ins>\n",
    "1. Load the raw scraped data containing:\n",
    "   - Body content\n",
    "   - Meta tags: `Meta_Title`, `Meta_Description`, and `Meta_Keywords`.\n",
    "2. Check for missing or duplicate values in all columns.\n",
    "3. Clean the raw content:\n",
    "   - Remove HTML artifacts, excess whitespace, and irrelevant characters.\n",
    "4. Standardize text in `Content` and meta tags:\n",
    "   - Convert to lowercase.\n",
    "   - Remove punctuation and special characters.\n",
    "5. Remove stopwords from textual columns.\n",
    "6. Tokenize the cleaned content and meta tags for further analysis.\n",
    "7. Save the preprocessed data for exploratory analysis and advanced preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "## <ins>Imports</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365c3333-82ea-4a9e-a279-6a76f1d13a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "# Uncomment rows below and execute if nltk stopwords and nltk punctuation not downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0a800-c046-4ccf-87d9-ffd3448f2720",
   "metadata": {},
   "source": [
    "## <ins>Load Data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800d7a22-bd6a-417f-805e-e74000ba173c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"data/scraped_data_with_meta.pkl\"\n",
    "raw_dataframe = pd.read_pickle(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e520aa-8466-49f7-b450-dd8a18dc37b1",
   "metadata": {},
   "source": [
    "## Step 2: Checking for Missing and Duplicate Data\n",
    "\n",
    "This step ensures the data integrity by identifying and handling:\n",
    "- Missing values (e.g., null or empty fields).\n",
    "- Remove any columns with more than 70% of the date have a null or empty field\n",
    "- Duplicate entries to avoid redundancy in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ba05ed-61d1-4932-ba47-e5a0a86f2a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing and Empty Value Summary:\n",
      "                  Total Rows  Missing Values  Empty Strings  \\\n",
      "URL                     36.0             0.0            0.0   \n",
      "Content                 36.0             0.0            0.0   \n",
      "Meta_Title              36.0             0.0            0.0   \n",
      "Meta_Description        36.0             0.0            3.0   \n",
      "Meta_Keywords           36.0             0.0           34.0   \n",
      "\n",
      "                  Total Missing/Empty  Percentage Missing/Empty  \n",
      "URL                               0.0                  0.000000  \n",
      "Content                           0.0                  0.000000  \n",
      "Meta_Title                        0.0                  0.000000  \n",
      "Meta_Description                  3.0                  8.333333  \n",
      "Meta_Keywords                    34.0                 94.444444  \n"
     ]
    }
   ],
   "source": [
    "def check_missing_and_empty(dataframe):\n",
    "    summary = {}\n",
    "    for column in dataframe.columns:\n",
    "        total = len(dataframe)\n",
    "        missing = dataframe[column].isnull().sum()\n",
    "        empty = (dataframe[column] == \"\").sum()\n",
    "        summary[column] = {\n",
    "            \"Total Rows\": total,\n",
    "            \"Missing Values\": missing,\n",
    "            \"Empty Strings\": empty,\n",
    "            \"Total Missing/Empty\": missing + empty,\n",
    "            \"Percentage Missing/Empty\": ((missing + empty) / total) * 100\n",
    "        }\n",
    "    return pd.DataFrame(summary).T  # Convert summary to a DataFrame for readability\n",
    "\n",
    "# Check missing and empty values for all columns\n",
    "missing_summary = check_missing_and_empty(raw_dataframe)\n",
    "\n",
    "print(\"Missing and Empty Value Summary:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9743c952-7502-4648-8560-8d0571c31a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with more than 70% missing/empty values: ['Meta_Keywords']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with 70%+ missing/empty values\n",
    "threshold = 70  \n",
    "columns_to_drop = missing_summary[\n",
    "    missing_summary[\"Percentage Missing/Empty\"] > threshold\n",
    "].index.tolist()\n",
    "\n",
    "if columns_to_drop:\n",
    "    print(f\"Dropping columns with more than {threshold}% missing/empty values: {columns_to_drop}\")\n",
    "    raw_dataframe.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"No columns to drop based on missing/empty values threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0735b69-8fbb-45ab-bdd9-2ad51e293739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove duplicates based on the URL column\n",
    "raw_dataframe.drop_duplicates(subset=\"URL\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a8b1c-2bc9-4e4d-86eb-d3f3456632d1",
   "metadata": {},
   "source": [
    "## Step 3: Cleaning Raw Content\n",
    "\n",
    "The raw scraped content often contains unwanted elements such as HTML tags, scripts, styles, and excess whitespace. This step focuses on:\n",
    "- Removing all HTML tags to retain plain text.\n",
    "- Eliminating any lingering JavaScript or CSS code.\n",
    "- Cleaning up newline characters, tabs, and extra spaces for a cleaner dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e99f88d-2fbe-4ff2-89a6-90f74e3b4c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text in 'Content', 'Meta_Title', and 'Meta_Description' columns, including JS and CSS removal.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean text, including removing JS and CSS\n",
    "def clean_text_with_js_css_removal(text):\n",
    "    if isinstance(text, str):\n",
    "        # Parse the text using BeautifulSoup\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        \n",
    "        # Remove <script> and <style> tags along with their content\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "        \n",
    "        # Get plain text from the HTML\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Remove special characters, punctuation, and extra spaces\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove special characters and punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Replace multiple spaces with a single space\n",
    "        return text.lower()\n",
    "    return text  # Return as is if not a string\n",
    "\n",
    "# Apply cleaning to relevant columns\n",
    "columns_to_clean = [\"Content\", \"Meta_Title\", \"Meta_Description\"]\n",
    "\n",
    "for column in columns_to_clean:\n",
    "    raw_dataframe[column] = raw_dataframe[column].apply(clean_text_with_js_css_removal)\n",
    "\n",
    "print(\"Cleaned text in 'Content', 'Meta_Title', and 'Meta_Description' columns, including JS and CSS removal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc37804-3e4f-40e0-9efa-ae262645de30",
   "metadata": {},
   "source": [
    "## Step 4: Standardizing Text\n",
    "\n",
    "Standardizing text ensures uniformity in formatting, which is essential for further processing like tokenization and analysis. This step includes:\n",
    "- Converting all text to lowercase for consistency.\n",
    "- Stripping any leading or trailing whitespace.\n",
    "- Removing unnecessary newline characters or tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1097cb69-2beb-43e4-b2fe-fc681cdc30c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized text in 'Content', 'Meta_Title', and 'Meta_Description' columns.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to standardize text\n",
    "def standardize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().strip()  # Convert to lowercase and remove leading/trailing whitespace\n",
    "    return text  # Return as is if not a string\n",
    "\n",
    "# Apply standardization to relevant columns\n",
    "columns_to_standardize = [\"Content\", \"Meta_Title\", \"Meta_Description\"]\n",
    "\n",
    "for column in columns_to_standardize:\n",
    "    raw_dataframe[column] = raw_dataframe[column].apply(standardize_text)\n",
    "\n",
    "print(\"Standardized text in 'Content', 'Meta_Title', and 'Meta_Description' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d58a9-3c0e-4826-8b42-0ea71a37440c",
   "metadata": {},
   "source": [
    "## Step 5: Removing General Stopwords\n",
    "In this step, we remove standard English stopwords to simplify the text and reduce noise. We will not apply any custom or domain-specific stopword removal at this stage, as this will be addressed during exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b7f9e3-bc5f-4cb5-8366-c853e33097b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed general stopwords from 'Content', 'Meta_Title', and 'Meta_Description' columns.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_general_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "# Apply general stopword removal to the relevant columns\n",
    "raw_dataframe[\"Content\"] = raw_dataframe[\"Content\"].apply(remove_general_stopwords)\n",
    "raw_dataframe[\"Meta_Title\"] = raw_dataframe[\"Meta_Title\"].apply(remove_general_stopwords)\n",
    "raw_dataframe[\"Meta_Description\"] = raw_dataframe[\"Meta_Description\"].apply(remove_general_stopwords)\n",
    "\n",
    "print(\"Removed general stopwords from 'Content', 'Meta_Title', and 'Meta_Description' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415b521-0ac2-4be9-ae24-627613835a70",
   "metadata": {},
   "source": [
    "## Step 6: Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual components, such as words or sentences. For our analysis, we will tokenize the content into words. This step prepares the data for further preprocessing and text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ad27eb-d676-461d-863f-9cbd265d8805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 'Content', 'Meta_Title', and 'Meta_Description' columns into words.\n"
     ]
    }
   ],
   "source": [
    "# Define a function for tokenization\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    return text\n",
    "\n",
    "# Apply tokenization to the relevant columns\n",
    "raw_dataframe[\"Tokenized_Content\"] = raw_dataframe[\"Content\"].apply(tokenize_text)\n",
    "raw_dataframe[\"Tokenized_Meta_Title\"] = raw_dataframe[\"Meta_Title\"].apply(tokenize_text)\n",
    "raw_dataframe[\"Tokenized_Meta_Description\"] = raw_dataframe[\"Meta_Description\"].apply(tokenize_text)\n",
    "\n",
    "print(\"Tokenized 'Content', 'Meta_Title', and 'Meta_Description' columns into words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fecdb83-3094-4eb1-9abd-77db1d968d23",
   "metadata": {},
   "source": [
    "## Step 7: Save Initial Preprocessed Data\n",
    "\n",
    "At this stage, the data has been cleaned and tokenized, making it ready for exploratory data analysis (EDA) and advanced preprocessing. The processed data will be saved as a `.pkl` file for seamless loading in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7955bc-ac99-4e44-b5b0-bc5da28ddd38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the preprocessed data for further analysis.\n"
     ]
    }
   ],
   "source": [
    "# Save the updated preprocessed data\n",
    "raw_dataframe.to_pickle(\"data/initial_preprocessed_data.pkl\")\n",
    "raw_dataframe.to_csv(\"data/initial_preprocessed_data.csv\", index=False)\n",
    "\n",
    "print(\"Saved the preprocessed data for further analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
